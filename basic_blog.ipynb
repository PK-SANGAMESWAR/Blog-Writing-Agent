{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88f6239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1474261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Schema Definitions ---\n",
    "\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    goal: str = Field(..., description=\"One sentence goal.\")\n",
    "    bullets: str = Field(..., min_length=3, max_length=5, description=\"3-5 points.\")\n",
    "    target_words: str = Field(..., description=\"120-450 words.\")\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(..., description=\"Use 'common_mistakes' exactly once.\")\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description=\"Who this blog is for\")\n",
    "    tone: str = Field(..., description=\"Writing tone (e.g., practical, crisp)\")\n",
    "    tasks: List[Task]\n",
    "\n",
    "Plan.model_rebuild()\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # Annotated with operator.add allows workers to append to this list in parallel\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5:7b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6e5d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Node Functions ---\n",
    "\n",
    "def orchestrator(state: State) -> dict:\n",
    "    # Use structured output to get the Plan object\n",
    "    structured_llm = llm.with_structured_output(Plan)\n",
    "    plan = structured_llm.invoke([\n",
    "        SystemMessage(content=(\n",
    "        \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 5–7 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120–450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\")),\n",
    "        HumanMessage(content=f\"Topic: {state['topic']}\")\n",
    "    ])\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "def fanout(state: State):\n",
    "    # This creates a 'Send' object for every task in the plan\n",
    "    return [\n",
    "        Send(\"worker\", {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n",
    "\n",
    "def worker(payload: dict) -> dict:\n",
    "    # 1. Properly unpack the payload\n",
    "    task: Task = payload[\"task\"]\n",
    "    plan: Plan = payload[\"plan\"]\n",
    "    topic = payload[\"topic\"]\n",
    "\n",
    "    # 2. System message is already great, just call it\n",
    "    system_prompt = (\n",
    "        \"You are a senior technical writer and developer advocate. Write ONE section \"\n",
    "        \"of a technical blog post in Markdown.\\n\\n\"\n",
    "        \"Hard constraints:\\n\"\n",
    "        \"- Follow the Goal and cover ALL Bullets.\\n\"\n",
    "        \"- Stay close to Target words (±15%).\\n\"\n",
    "        \"- Output ONLY Markdown (start with ## Section Title).\\n\"\n",
    "        \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "        \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "        \"- When relevant, include at least one of:\\n\"\n",
    "        \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "        \"  * a tiny example input/output\\n\"\n",
    "        \"  * a checklist of steps\\n\"\n",
    "        \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "        \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "        \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "        \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "        \"Markdown style:\\n\"\n",
    "        \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "        \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "        \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "        \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "    )\n",
    "\n",
    "    # 3. Use the actual variables in the Human Message\n",
    "    human_prompt = (\n",
    "        f\"Blog Title: {plan.blog_title}\\n\"\n",
    "        f\"Audience: {plan.audience}\\n\"\n",
    "        f\"Tone: {plan.tone}\\n\"\n",
    "        f\"Topic: {topic}\\n\\n\"\n",
    "        f\"Section: {task.title}\\n\"\n",
    "        f\"Goal: {task.goal}\\n\"\n",
    "        f\"Bullets: {task.bullets}\\n\"\n",
    "        f\"Target words: {task.target_words}\"\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=human_prompt)\n",
    "    ])\n",
    "\n",
    "    # 4. IMPORTANT: Return the ID so the reducer can sort the sections\n",
    "    return {\"sections\": [(task.id, response.content.strip())]}\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    # 1. Sort sections by their ID (index 0 of the tuple)\n",
    "    # state['sections'] will look like: [(3, \"text\"), (1, \"text\"), (2, \"text\")]\n",
    "    ordered_list = sorted(state[\"sections\"], key=lambda x: x[0])\n",
    "    \n",
    "    # 2. Extract just the text\n",
    "    body = \"\\n\\n\".join([content for _id, content in ordered_list])\n",
    "    \n",
    "    title = state[\"plan\"].blog_title\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = f\"{title.lower().replace(' ', '_')}.md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "325947f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Graph Construction ---\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"orchestrator\", orchestrator)\n",
    "workflow.add_node(\"worker\", worker)\n",
    "workflow.add_node(\"reducer\", reducer)\n",
    "\n",
    "workflow.add_edge(START, \"orchestrator\")\n",
    "# Conditional edge handles the fan-out logic\n",
    "workflow.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "workflow.add_edge(\"worker\", \"reducer\")\n",
    "workflow.add_edge(\"reducer\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "995f31d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001C2866EB2C0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe28c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\":\"Write a blog on Self Attention\", \"sections\" : []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a36794d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog_title='Mastering Self-Attention in Transformers: A Developer’s Guide' audience='Developers familiar with neural networks, particularly those working on NLP tasks.' tone='Technical and actionable' tasks=[Task(id=1, title='Introduction to Self Attention Mechanism', goal='Understand the importance of self-attention in transformers for handling sequential data effectively.', bullets='urls,', target_words='[300]', section_type='intro'), Task(id=2, title='Problem: Handling Long Sequences in Neural Networks', goal='Recognize the limitations of traditional RNN and CNN models when dealing with long sequences.', bullets='List ', target_words='[450]', section_type='core'), Task(id=3, title='Intuition Behind Self-Attention', goal='Grasp the core intuition behind self-attention and how it enables parallelization in sequence modeling.', bullets='Extr', target_words='[400]', section_type='core'), Task(id=4, title='Approach: Implementing Self-Attention Mechanism', goal='Implement a simple self-attention mechanism from scratch.', bullets='Show ', target_words='[500]', section_type='core'), Task(id=5, title='Trade-offs in Self-Attention', goal='Understand the computational and memory trade-offs involved in implementing self-attention.', bullets='List ', target_words='[300]', section_type='core'), Task(id=6, title='Testing and Observability', goal='Set up a testing framework to ensure the self-attention mechanism works correctly under various conditions.', bullets='Add ', target_words='[300]', section_type='core'), Task(id=7, title='Common Mistakes in Implementing Self-Attention', goal='Avoid common pitfalls when implementing self-attention mechanisms.', bullets='MWE, ', target_words='[200]', section_type='common_mistakes'), Task(id=8, title='Advanced Topics: Self-Attention Variants', goal='Explore advanced self-attention variants like Multi-Head Attention and their benefits.', bullets='MWE, ', target_words='[350]', section_type='core'), Task(id=9, title='Conclusion: Next Steps', goal='Get a clear roadmap for further exploration and practical application of self-attention in your projects.', bullets='Sum', target_words='[200]', section_type='conclusion')]\n"
     ]
    }
   ],
   "source": [
    "print(out[\"plan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb84ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Mastering Self-Attention in Transformers: A Developer’s Guide\n",
      "\n",
      "## Introduction to Self Attention Mechanism\n",
      "\n",
      "Self-Attention, a key component in transformer models, allows each position in a sequence to attend to all positions in that same sequence. This mechanism is crucial for understanding the context and dependencies within sequences, making it indispensable for tasks like language modeling and text translation.\n",
      "\n",
      "### Why Self Attention Matters\n",
      "\n",
      "In traditional recurrent neural networks (RNNs), information flows through sequential processing, which can be slow and inefficient. Transformers, on the other hand, leverage self-attention to process all elements in parallel, significantly speeding up computation while maintaining or even improving performance. The self-attention mechanism computes a weighted sum of the query, key, and value vectors for each position, enabling the model to weigh different parts of the sequence more heavily based on their relevance.\n",
      "\n",
      "### Basic Mechanics\n",
      "\n",
      "The core operation of self-attention involves three vectors: the Query (Q), Key (K), and Value (V). Each element in these vectors is computed from the corresponding element in the input sequence. The attention score for each pair of elements \\( (q_i, k_j) \\) is calculated using a similarity function, typically a dot product:\n",
      "\n",
      "\\[ \\text{Score}(q_i, k_j) = q_i \\cdot k_j^T \\]\n",
      "\n",
      "The final value vector is obtained by applying a weighted sum based on these scores. Mathematically, the self-attention mechanism can be represented as:\n",
      "\n",
      "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
      "\n",
      "Where \\( d_k \\) is the dimension of the key vectors.\n",
      "\n",
      "### Example: Computing Self-Attention\n",
      "\n",
      "Let's consider a small example with 3 words and each word represented by a vector of length 4:\n",
      "\n",
      "**Input Sequence:** \"The cat sat\"\n",
      "\n",
      "- **Query (Q):** \\[ [1, 0.5, -1, 2], [-0.7, 1, 0.5, -1], [1, 1, 0, 0] \\]\n",
      "- **Key (K):** \\[ [2, 0.3, -0.8, 0.4], [0.9, 1, -1, 0.6], [-0.5, -0.7, 1, 0.3] \\]\n",
      "- **Value (V):** \\[ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12] \\]\n",
      "\n",
      "The attention scores are computed as:\n",
      "\n",
      "\\[ \n",
      "\\text{Scores} = QK^T / \\sqrt{4} = \\left[ \\begin{array}{ccc}\n",
      "-0.5 & -1.3 & -0.8 \\\\\n",
      "-0.9 & 1.6 & -2.7 \\\\\n",
      "0.5 & 2.6 & -2.5\n",
      "\\end{array} \\right]\n",
      "\\]\n",
      "\n",
      "Applying the softmax function, we get:\n",
      "\n",
      "\\[ \n",
      "\\text{Softmax Scores} = \\left[ \\begin{array}{ccc}\n",
      "0.247 & 0.139 & 0.083 \\\\\n",
      "0.065 & 0.117 & 0.020 \\\\\n",
      "0.688 & 0.744 & 0.897\n",
      "\\end{array} \\right]\n",
      "\\]\n",
      "\n",
      "The final self-attention output is computed by weighting the values:\n",
      "\n",
      "\\[ \n",
      "\\text{Output} = \\text{Softmax Scores} \\times V = \\left[ \\begin{array}{c}\n",
      "2.61 \\\\\n",
      "4.65 \\\\\n",
      "9.87\n",
      "\\end{array} \\right]\n",
      "\\]\n",
      "\n",
      "### Trade-offs\n",
      "\n",
      "Self-attention introduces some trade-offs:\n",
      "- **Complexity:** While parallel processing speeds up computation, the time complexity of self-attention is \\( O(n^2) \\), where \\( n \\) is the sequence length.\n",
      "- **Memory Usage:** The space required for keys and values increases with the size of the input sequences.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Self-attention is a powerful mechanism that allows transformers to understand context effectively. By enabling each position in a sequence to weigh all other positions, it significantly improves model performance on sequential data tasks. Understanding self-attention is crucial for developers working on NLP applications or any domain requiring efficient processing of sequences.\n",
      "\n",
      "## Problem: Handling Long Sequences in Neural Networks\n",
      "\n",
      "### Limitations of Traditional Models\n",
      "\n",
      "When working with neural networks, especially for tasks involving natural language processing (NLP), one critical challenge is handling long sequences efficiently. Traditional models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have inherent limitations that make them suboptimal for such tasks.\n",
      "\n",
      "#### RNN Limitations\n",
      "\n",
      "- **Vanishing/Exploding Gradients**: In RNNs, the gradient information can diminish or explode as it is propagated through time steps. This makes training challenging, particularly with long sequences.\n",
      "  \n",
      "#### CNN Limitations\n",
      "\n",
      "- **Local Connectivity**: CNNs process local neighborhoods in the input data but struggle with capturing long-range dependencies due to their fixed receptive fields.\n",
      "\n",
      "### Introducing Self-Attention Mechanism\n",
      "\n",
      "Self-attention mechanisms address these limitations by allowing each position in a sequence to attend over all positions. This global awareness makes self-attention particularly well-suited for handling long sequences effectively.\n",
      "\n",
      "#### Self-Attention Basics\n",
      "\n",
      "The key idea behind self-attention is that the output at any given time step can be influenced by the context provided by other elements in the same input sequence. Mathematically, this can be represented as:\n",
      "\n",
      "\\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]\n",
      "\n",
      "Where:\n",
      "- \\( Q \\), \\( K \\), and \\( V \\) are the query, key, and value matrices.\n",
      "- \\( d_k \\) is the dimension of keys.\n",
      "\n",
      "#### Why Self-Attention?\n",
      "\n",
      "Self-attention allows models to capture dependencies at any distance within a sequence, making it more efficient for long sequences compared to RNNs and CNNs. This global context awareness also leads to better performance in tasks like machine translation, text summarization, and question answering.\n",
      "\n",
      "### Implementing Self-Attention\n",
      "\n",
      "To implement self-attention in your model, you can use the following steps:\n",
      "\n",
      "1. **Compute Query (Q), Key (K), and Value (V) Matrices**:\n",
      "   - Typically, these are derived from the input sequence.\n",
      "   \n",
      "2. **Calculate Attention Scores**:\n",
      "   \\[ \\text{Scores} = QK^T / \\sqrt{d_k} \\]\n",
      "\n",
      "3. **Apply Softmax to Normalize Scores**:\n",
      "   \\[ \\text{Attention Weights} = \\text{softmax}(\\text{Scores}) \\]\n",
      "\n",
      "4. **Compute the Weighted Sum of Values (V)**:\n",
      "   \\[ \\text{Output} = \\text{Attention Weights} V \\]\n",
      "\n",
      "Here’s a minimal code snippet to illustrate:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "def self_attention(query, key, value):\n",
      "    d_k = query.size(-1)\n",
      "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
      "    attention_weights = F.softmax(scores, dim=-1)\n",
      "    output = torch.matmul(attention_weights, value)\n",
      "    return output\n",
      "```\n",
      "\n",
      "### Trade-offs and Best Practices\n",
      "\n",
      "- **Performance**: Self-Attention significantly improves the handling of long sequences but can be computationally expensive. This is especially true for very large models.\n",
      "  \n",
      "- **Complexity**: Implementing self-attention requires careful consideration of how to manage memory usage, particularly in deep networks.\n",
      "\n",
      "- **Reliability**: Ensuring numerical stability during training and inference is crucial due to the division by \\( \\sqrt{d_k} \\).\n",
      "\n",
      "### Edge Cases\n",
      "\n",
      "- **Zero Division Error**: Ensure that \\( d_k \\) does not become zero. Typically, this can be managed by setting a minimum value for \\( d_k \\).\n",
      "  \n",
      "- **Infinite Attention Scores**: Softmax can sometimes result in very high scores, leading to significant weights on certain tokens. Regularization techniques like dropout or adding a small constant to the denominator can help mitigate these issues.\n",
      "\n",
      "By understanding and implementing self-attention effectively, you can significantly enhance your model’s ability to handle long sequences in NLP tasks while maintaining efficiency and accuracy.\n",
      "\n",
      "## Intuition Behind Self-Attention\n",
      "\n",
      "Self-attention, a key component of transformers, revolutionizes how neural networks handle sequential data. Understanding its core intuition is crucial for leveraging its power effectively.\n",
      "\n",
      "### How Self-Attention Enables Parallelization\n",
      "\n",
      "The essence of self-attention lies in allowing each position in the sequence to attend to all other positions. This fundamentally changes the way information flows through a model:\n",
      "\n",
      "1. **Sequential Processing vs. Parallel Attention**:\n",
      "   - In traditional sequential models (e.g., RNNs), processing elements in a sequence is inherently serial, with each step depending on the previous one.\n",
      "   - Self-attention, however, computes an attention score for every pair of positions in the sequence simultaneously. This parallel computation significantly speeds up training and inference.\n",
      "\n",
      "2. **Attention Mechanism**:\n",
      "   - Each position \\( i \\) generates queries (\\( Q_i \\)), keys (\\( K_i \\)), and values (\\( V_i \\)).\n",
      "   - These are then used to compute attention scores, which determine the weight each element should have when computing the final representation at each position.\n",
      "\n",
      "3. **Mathematical Formulation**:\n",
      "   - Attention score \\( A(i, j) \\) is computed as a function of the query \\( Q_i \\), key \\( K_j \\), and value \\( V_j \\):\n",
      "     ```plaintext\n",
      "     A(i, j) = f(Q_i * K_j)\n",
      "     ```\n",
      "   - The final attention weighted sum is calculated by:\n",
      "     ```plaintext\n",
      "     Output = ∑(A(i, j) * V_j)\n",
      "     ```\n",
      "\n",
      "4. **Diagram of Attention Computation**:\n",
      "   - Flow: \\( Q \\rightarrow K \\rightarrow A \\rightarrow V \\rightarrow Output \\)\n",
      "\n",
      "### Practical Example\n",
      "\n",
      "Consider a simple sequence \"Hello World\". Each word is represented by a vector:\n",
      "\n",
      "```plaintext\n",
      "Q = [q1, q2, q3, q4, q5]\n",
      "K = [k1, k2, k3, k4, k5]\n",
      "V = [v1, v2, v3, v4, v5]\n",
      "```\n",
      "\n",
      "The attention score for each pair is computed:\n",
      "\n",
      "```plaintext\n",
      "A(1, 1) = f(q1 * k1)\n",
      "A(1, 2) = f(q1 * k2)\n",
      "...\n",
      "A(5, 5) = f(q5 * k5)\n",
      "```\n",
      "\n",
      "These scores are then used to weight the values \\( V \\):\n",
      "\n",
      "```plaintext\n",
      "Output = A(1, 1) * v1 + A(1, 2) * v2 + ... + A(5, 5) * v5\n",
      "```\n",
      "\n",
      "### Trade-offs and Best Practices\n",
      "\n",
      "- **Complexity**: While self-attention significantly speeds up the model, it introduces complexity in terms of memory usage and computational requirements.\n",
      "- **Implementation**: Opt for efficient implementations that use techniques like sparse attention to reduce computation where possible.\n",
      "\n",
      "By grasping this intuition, developers can better design and optimize models using self-attention mechanisms.\n",
      "\n",
      "## Approach: Implementing the Self-Attention Mechanism\n",
      "\n",
      "Implementing a self-attention mechanism is crucial for understanding how transformers process information. This section will guide you through creating a basic implementation of self-attention, focusing on clarity and simplicity.\n",
      "\n",
      "### Setting Up Your Environment\n",
      "\n",
      "Before diving into coding, ensure your development environment includes Python 3.x with the following libraries installed:\n",
      "\n",
      "```bash\n",
      "pip install numpy torch\n",
      "```\n",
      "\n",
      "### Understanding Self-Attention\n",
      "\n",
      "Self-attention allows each position in the sequence to attend over all positions of the same sequence. This mechanism is implemented using three weight matrices: `Q` (query), `K` (key), and `V` (value). The attention scores are computed by taking the dot product between the query and key vectors, followed by a softmax operation.\n",
      "\n",
      "### Implementing Self-Attention\n",
      "\n",
      "Let's start with importing necessary libraries:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import numpy as np\n",
      "```\n",
      "\n",
      "#### Step 1: Define the Self-Attention Module\n",
      "\n",
      "We will define a class `SelfAttention` to encapsulate our implementation. The constructor initializes the weight matrices, and the forward pass computes the attention scores and outputs.\n",
      "\n",
      "```python\n",
      "class SelfAttention(torch.nn.Module):\n",
      "    def __init__(self, embed_dim):\n",
      "        super(SelfAttention, self).__init__()\n",
      "        self.query = torch.nn.Linear(embed_dim, embed_dim)\n",
      "        self.key = torch.nn.Linear(embed_dim, embed_dim)\n",
      "        self.value = torch.nn.Linear(embed_dim, embed_dim)\n",
      "    \n",
      "    def forward(self, x):\n",
      "        query = self.query(x)  # (batch_size, seq_len, embed_dim)\n",
      "        key = self.key(x)      # (batch_size, seq_len, embed_dim)\n",
      "        value = self.value(x)  # (batch_size, seq_len, embed_dim)\n",
      "        \n",
      "        scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(embed_dim)  # (batch_size, seq_len, seq_len)\n",
      "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
      "        \n",
      "        context_vector = torch.matmul(attention_weights, value)  # (batch_size, seq_len, embed_dim)\n",
      "        return context_vector\n",
      "```\n",
      "\n",
      "#### Step 2: Testing the Implementation\n",
      "\n",
      "Let's test our `SelfAttention` module with a simple input sequence.\n",
      "\n",
      "```python\n",
      "embed_dim = 64\n",
      "seq_len = 5\n",
      "batch_size = 3\n",
      "\n",
      "x = torch.randn(batch_size, seq_len, embed_dim)  # (batch_size, seq_len, embed_dim)\n",
      "\n",
      "attention_module = SelfAttention(embed_dim)\n",
      "output = attention_module(x)\n",
      "\n",
      "print(output.shape)  # Expected: (3, 5, 64)\n",
      "```\n",
      "\n",
      "### Trade-offs and Best Practices\n",
      "\n",
      "- **Performance**: Using `torch.nn.Linear` for simplicity. For larger models, consider using more efficient implementations like `nn.Linear.bias=False`.\n",
      "- **Cost**: Softmax is computationally expensive due to its O(n^2) complexity. Consider approximations or alternatives like scaled dot-product attention.\n",
      "- **Complexity**: The implementation focuses on clarity but may not be the most optimized version. Further optimizations are possible by leveraging parallelization techniques.\n",
      "\n",
      "### Edge Cases\n",
      "\n",
      "- Ensure that `embed_dim` is a positive integer.\n",
      "- Handle cases where input sequences have varying lengths, which might require padding or dynamic computation of attention scores.\n",
      "\n",
      "By following these steps and considering the trade-offs, you can effectively implement and use self-attention in your transformer models.\n",
      "\n",
      "## Trade-offs in Self-Attention\n",
      "\n",
      "Self-attention mechanisms are fundamental to transformers, enabling them to focus on different parts of input sequences. However, these mechanisms come with significant computational and memory trade-offs.\n",
      "\n",
      "### Computational Complexity\n",
      "\n",
      "Self-attention involves computing a weighted sum over all pairs of elements within a sequence. The complexity of this operation is \\( O(n^2) \\), where \\( n \\) is the length of the sequence. This quadratic scaling can be prohibitive for long sequences, as seen in language modeling tasks like machine translation or text summarization.\n",
      "\n",
      "#### Example Input/Output\n",
      "\n",
      "Consider an input sequence with 100 tokens:\n",
      "\n",
      "```plaintext\n",
      "Input: [token_1, token_2, ..., token_100]\n",
      "```\n",
      "\n",
      "For each position in the sequence, self-attention computes scores for every pair of tokens. This results in a dense attention matrix of size \\( 100 \\times 100 \\).\n",
      "\n",
      "### Memory Trade-offs\n",
      "\n",
      "The memory required to store these dense matrices further exacerbates the computational burden. For sequences longer than about 500 tokens, the dense representation may consume too much memory, leading to out-of-memory errors.\n",
      "\n",
      "#### Example Scenario: Out-of-Memory Error\n",
      "\n",
      "For a sequence of length \\( n = 1024 \\):\n",
      "\n",
      "- The attention matrix will be \\( 1024 \\times 1024 \\), consuming approximately \\( 8 MB \\) (assuming each token is represented by a 32-bit float).\n",
      "\n",
      "### Optimization Techniques\n",
      "\n",
      "To mitigate these trade-offs, several optimization techniques can be employed:\n",
      "\n",
      "- **Masked Self-Attention**: For tasks like language modeling where the future tokens should not influence the current prediction, masked self-attention can reduce computations.\n",
      "  \n",
      "- **Sparse Attention**: Methods such as BytePair Embeddings (BPE) or Span-Constrained Attention help to make attention sparse and thus more memory-efficient. Sparse attention constructs a sparse matrix that only includes relevant pairs of tokens.\n",
      "\n",
      "#### Example: Sparse Attention\n",
      "\n",
      "Consider a sequence with 100 tokens, but using BPE, where each token is broken down into sub-tokens:\n",
      "\n",
      "```plaintext\n",
      "Input: [sub_token_1_a, sub_token_1_b, ..., sub_token_100_c]\n",
      "```\n",
      "\n",
      "In this case, the attention matrix size can be significantly reduced compared to the original sequence length.\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "To balance computational efficiency and memory usage:\n",
      "\n",
      "- **Use Sparse Attention Mechanisms**: For long sequences, using techniques like BPE or span-constrained attention can help reduce computation.\n",
      "- **Optimize Attention Scores**: Implementing efficient algorithms for computing attention scores, such as parallelization on GPUs, can improve performance.\n",
      "\n",
      "By understanding these trade-offs, developers can make informed decisions to optimize their transformer models for specific tasks and constraints.\n",
      "\n",
      "## Testing and Observability\n",
      "\n",
      "To ensure that your self-attention mechanism functions as expected, it's crucial to set up a robust testing framework. This section will guide you through creating tests for the self-attention layer in a transformer model, focusing on edge cases and performance under various conditions.\n",
      "\n",
      "### Setting Up Tests\n",
      "\n",
      "1. **Install Necessary Libraries**\n",
      "   Ensure you have the necessary libraries installed:\n",
      "   ```sh\n",
      "   pip install torch pytest\n",
      "   ```\n",
      "\n",
      "2. **Define the Self-Attention Mechanism**\n",
      "   Suppose your self-attention mechanism is implemented as follows:\n",
      "\n",
      "   ```python\n",
      "   import torch\n",
      "   from torch.nn import Module\n",
      "\n",
      "   class SelfAttention(Module):\n",
      "       def __init__(self, embed_dim: int, heads: int):\n",
      "           super().__init__()\n",
      "           self.embed_dim = embed_dim\n",
      "           self.heads = heads\n",
      "           self.query = torch.nn.Linear(embed_dim, embed_dim)\n",
      "           self.key = torch.nn.Linear(embed_dim, embed_dim)\n",
      "           self.value = torch.nn.Linear(embed_dim, embed_dim)\n",
      "\n",
      "       def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
      "           # Implement the forward pass of self-attention\n",
      "           pass\n",
      "\n",
      "   model = SelfAttention(embed_dim=512, heads=8)\n",
      "   ```\n",
      "\n",
      "3. **Create a Testing Framework**\n",
      "   Write tests using `pytest` to validate different scenarios.\n",
      "\n",
      "   ```python\n",
      "   import pytest\n",
      "   from your_transformer_module import SelfAttention  # Adjust the import path as needed\n",
      "\n",
      "   def test_self_attention():\n",
      "       model = SelfAttention(embed_dim=512, heads=8)\n",
      "       \n",
      "       # Test with a batch of sequences\n",
      "       input_tensor = torch.randn(32, 10, 512)  # Batch size x Sequence length x Embedding dimension\n",
      "       output = model(input_tensor)\n",
      "       assert output.shape == (32, 10, 512), \"Output shape mismatch\"\n",
      "       \n",
      "       # Test edge case: zero vectors in input\n",
      "       zero_vector = torch.zeros(1, 512)\n",
      "       inputs_with_zeros = torch.cat((input_tensor[:2], zero_vector.unsqueeze(0), input_tensor[3:]), dim=0)\n",
      "       output_zeros = model(inputs_with_zeros)\n",
      "       assert (output_zeros[:, -1] == 0).all(), \"Output should be zero for zero vectors\"\n",
      "\n",
      "   if __name__ == \"__main__\":\n",
      "       pytest.main([__file__])\n",
      "   ```\n",
      "\n",
      "### Testing Scenarios\n",
      "\n",
      "- **Normal Case**: Ensure the self-attention mechanism processes a typical batch of input sequences without errors.\n",
      "  \n",
      "- **Edge Case: Zero Vectors**: Test how the model handles inputs containing zero vectors. The output for these should be zero to ensure no gradient flow.\n",
      "\n",
      "- **Large Sequences**: Verify that the self-attention layer can handle large sequence lengths without running into performance issues or memory leaks.\n",
      "\n",
      "### Example Flow\n",
      "\n",
      "1. **Input Preparation**\n",
      "   - Prepare a tensor with random values and another with zeros.\n",
      "   \n",
      "2. **Model Forward Pass**\n",
      "   - Feed the input tensors to the `SelfAttention` model.\n",
      "  \n",
      "3. **Assertion Checks**\n",
      "   - Assert that the output dimensions match expectations.\n",
      "   - Verify zero vector handling.\n",
      "\n",
      "### Trade-offs\n",
      "\n",
      "- **Performance**: Running tests on large sequences can be computationally intensive, so consider optimizing or parallelizing test cases if necessary.\n",
      "- **Complexity**: Writing comprehensive edge case tests increases code complexity but ensures robustness.\n",
      "  \n",
      "By setting up these tests, you can ensure that your self-attention mechanism is reliable and performs well under various conditions.\n",
      "\n",
      "## Common Mistakes in Implementing Self-Attention\n",
      "\n",
      "When implementing self-attention mechanisms, developers often encounter a few common pitfalls. Here’s how to avoid them and ensure your implementation is robust.\n",
      "\n",
      "### Misunderstanding the Query, Key, Value (QKV) Dimensions\n",
      "\n",
      "**Mistake:** Assuming that the dimensions of Q, K, and V should be identical, leading to incorrect calculations or errors in the attention scores.\n",
      "\n",
      "**Correct Approach:** The query matrix \\( \\mathbf{Q} \\), key matrix \\( \\mathbf{K} \\), and value matrix \\( \\mathbf{V} \\) are typically derived from the same input matrix but can have different dimensions. For example, if you have an input sequence of length \\( T \\) with embedding dimension \\( D \\):\n",
      "\n",
      "- **Q** is usually a matrix of shape \\( (T, D_{\\text{query}}) \\)\n",
      "- **K** and **V** are typically of shape \\( (T, D_{\\text{key}}) = (T, D_{\\text{value}}) = (T, D) \\)\n",
      "\n",
      "where \\( D_{\\text{query}} \\), \\( D_{\\text{key}} \\), and \\( D_{\\text{value}} \\) can be different but often equal.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Assume input embedding shape is (seq_len, embed_dim)\n",
      "input_embeddings = torch.randn((10, 512))\n",
      "\n",
      "# Define query, key, and value dimensions\n",
      "d_query = 64\n",
      "d_key = d_value = 512\n",
      "\n",
      "# Create QKV matrices\n",
      "Q = input_embeddings @ torch.randn(d_key, d_query).T\n",
      "K = input_embeddings @ torch.randn(d_key, d_key).T\n",
      "V = input_embeddings @ torch.randn(d_key, d_value).T\n",
      "\n",
      "# Attention scores calculation (simplified)\n",
      "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_key ** 0.5)\n",
      "\n",
      "# Apply softmax for probabilities\n",
      "attention_probs = torch.softmax(attention_scores, dim=-1)\n",
      "```\n",
      "\n",
      "**Trade-offs:** Using different dimensions can increase complexity but allows for more flexible modeling of attention mechanisms.\n",
      "\n",
      "### Ignoring the Masking in Attention\n",
      "\n",
      "**Mistake:** Not applying appropriate masks to prevent attention over irrelevant tokens, such as padding or special tokens.\n",
      "\n",
      "**Correct Approach:** Apply a causal mask to prevent future tokens from attending to past tokens. Additionally, apply a padding mask to avoid considering padded elements during scoring and softmax operations.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "# Create a causal mask (upper triangular matrix with zeros)\n",
      "def create_causal_mask(sz):\n",
      "    mask = torch.triu(torch.ones((sz, sz)), 1).type(torch.uint8)\n",
      "    return mask\n",
      "\n",
      "causal_mask = create_causal_mask(Q.size(0))\n",
      "\n",
      "# Apply the causal mask to the attention scores\n",
      "attention_scores.masked_fill_(causal_mask == 1, -float('inf'))\n",
      "\n",
      "# Create a padding mask (mask out padded tokens)\n",
      "def create_padding_mask(seq):\n",
      "    mask = seq != 0\n",
      "    return mask\n",
      "\n",
      "padding_mask = create_padding_mask(input_embeddings)\n",
      "```\n",
      "\n",
      "**Example Output:**\n",
      "```python\n",
      "attention_probs = torch.softmax(attention_scores, dim=-1)\n",
      "\n",
      "# Apply the padding mask to attention probabilities\n",
      "attention_probs = attention_probs * padding_mask.unsqueeze(-2).float()\n",
      "\n",
      "# Optionally, apply a normalization factor if necessary\n",
      "attention_probs = attention_probs / (attention_probs.sum(dim=-1, keepdim=True) + 1e-6)\n",
      "```\n",
      "\n",
      "### Overlooking the Softmax Operation\n",
      "\n",
      "**Mistake:** Failing to normalize the attention scores using softmax, which can lead to unstable or incorrect attention weights.\n",
      "\n",
      "**Correct Approach:** Apply a softmax function along the appropriate dimension to ensure that the attention scores sum up to one.\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_key ** 0.5)\n",
      "attention_probs = torch.softmax(attention_scores, dim=-1)\n",
      "```\n",
      "\n",
      "### Summary Checklist\n",
      "\n",
      "- Ensure Q, K, and V have appropriate dimensions.\n",
      "- Apply causal masks to prevent future-token attention.\n",
      "- Use padding masks to mask out irrelevant tokens.\n",
      "- Normalize using softmax to ensure valid probabilities.\n",
      "\n",
      "By following these guidelines, you can avoid common pitfalls and implement self-attention mechanisms more effectively.\n",
      "\n",
      "## Advanced Topics: Self-Attention Variants\n",
      "\n",
      "Self-attention mechanisms are fundamental to transformers, enabling them to weigh the importance of different tokens in a sequence. While standard self-attention is effective, more advanced variants offer enhanced capabilities and flexibility. In this section, we focus on Multi-Head Attention (MHA), which significantly expands the model's ability to capture complex relationships within sequences.\n",
      "\n",
      "### Why Use Multi-Head Attention?\n",
      "\n",
      "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. This enhances the model’s capacity to represent more complex functions and learn richer representations of input data, making it particularly beneficial for tasks requiring deep understanding of context in natural language processing (NLP).\n",
      "\n",
      "### Implementation Details\n",
      "\n",
      "Let's delve into how MHA works and its implementation.\n",
      "\n",
      "#### Flow: A -> B -> C\n",
      "1. **Multi-Head Attention**: Takes a query \\( Q \\), key \\( K \\), value \\( V \\) matrix, and splits them into multiple heads to handle different aspects of the input.\n",
      "2. **Concatenation and Linear Transformation**: Combines the outputs from all heads using linear transformations.\n",
      "3. **Output Layer**: Applies another linear transformation to produce the final output.\n",
      "\n",
      "#### Code Snippet\n",
      "Here’s a minimal implementation of Multi-Head Attention in PyTorch:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torch import nn\n",
      "\n",
      "class MultiHeadAttention(nn.Module):\n",
      "    def __init__(self, embed_dim, num_heads):\n",
      "        super(MultiHeadAttention, self).__init__()\n",
      "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
      "        \n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        self.head_dim = embed_dim // num_heads\n",
      "        \n",
      "        # Linear transformations for query, key, and value\n",
      "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
      "        \n",
      "        self.out = nn.Linear(embed_dim, embed_dim)\n",
      "\n",
      "    def forward(self, queries, keys, values):\n",
      "        batch_size = queries.shape[0]\n",
      "        \n",
      "        # Linear transformations\n",
      "        Q = self.q_linear(queries).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        K = self.k_linear(keys).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        V = self.v_linear(values).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
      "        \n",
      "        # Scaled dot-product attention\n",
      "        weights = torch.matmul(Q / (self.embed_dim ** 0.5), K.transpose(-2, -1))\n",
      "        attention = nn.functional.softmax(weights, dim=-1)\n",
      "        output = torch.matmul(attention, V)\n",
      "        \n",
      "        # Concatenate and apply final linear transformation\n",
      "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
      "        return self.out(output)\n",
      "\n",
      "# Example usage\n",
      "embed_dim = 512\n",
      "num_heads = 8\n",
      "mha = MultiHeadAttention(embed_dim, num_heads)\n",
      "queries = torch.randn((64, 32, embed_dim))\n",
      "keys = torch.randn((64, 64, embed_dim))\n",
      "values = torch.randn((64, 64, embed_dim))\n",
      "\n",
      "output = mha(queries, keys, values)\n",
      "print(output.shape)  # Expected shape: (64, 32, 512)\n",
      "```\n",
      "\n",
      "### Trade-offs and Considerations\n",
      "\n",
      "- **Performance**: MHA increases computational complexity due to the additional linear transformations. For large models or datasets, this can be a significant bottleneck.\n",
      "- **Cost**: More heads allow for better representation learning but require more parameters, which can increase training costs.\n",
      "- **Complexity**: Handling multiple attention heads adds complexity in model design and hyperparameter tuning.\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "- Start with a small number of heads (e.g., 8) to ensure the model is not over-parametrized.\n",
      "- Monitor performance on validation sets as you increase the number of heads to find an optimal balance between model capacity and computational efficiency.\n",
      "\n",
      "By leveraging Multi-Head Attention, developers can build more powerful transformers that handle complex NLP tasks with greater accuracy and flexibility.\n",
      "\n",
      "## Conclusion: Next Steps\n",
      "\n",
      "Now that you have a solid understanding of self-attention mechanisms, let's discuss the next steps to deepen your knowledge and apply this technique effectively in your projects.\n",
      "\n",
      "### 1. Implement Self-Attention in Your Projects\n",
      "\n",
      "To get hands-on experience with self-attention, start by integrating it into an existing NLP project or build a simple sequence-to-sequence model using libraries like TensorFlow or PyTorch. Here’s a small code snippet to get you started:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torch.nn import functional as F\n",
      "\n",
      "# Define the query, key, and value tensors\n",
      "query = torch.randn(10, 50, 64)  # (batch_size, seq_len, embed_dim)\n",
      "key = torch.randn(10, 50, 64)\n",
      "value = torch.randn(10, 50, 64)\n",
      "\n",
      "# Compute the attention scores\n",
      "scores = torch.matmul(query, key.transpose(-2, -1)) / (64 ** 0.5)  # scaled dot-product\n",
      "\n",
      "# Apply a softmax function to normalize the scores\n",
      "attention_weights = F.softmax(scores, dim=-1)\n",
      "\n",
      "# Apply the attention weights to the value tensor\n",
      "contextualized_representation = torch.matmul(attention_weights, value)\n",
      "```\n",
      "\n",
      "### 2. Experiment with Different Variants of Self-Attention\n",
      "\n",
      "Self-attention has several variations such as multi-head self-attention and relative positional encodings. Try out these variants by modifying your model architecture. For instance, in a transformer-based model:\n",
      "\n",
      "```python\n",
      "class MultiHeadSelfAttention(nn.Module):\n",
      "    def __init__(self, embed_dim, num_heads=8):\n",
      "        super(MultiHeadSelfAttention, self).__init__()\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
      "        \n",
      "        # Linear layers to project Q, K, V\n",
      "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
      "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
      "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        batch_size, seq_len, embed_dim = x.size()\n",
      "        \n",
      "        # Project queries, keys, and values\n",
      "        query = self.query_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n",
      "        key = self.key_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n",
      "        value = self.value_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n",
      "        \n",
      "        # Swap dimensions for multi-head parallelism\n",
      "        query, key, value = [x.transpose(1, 2) for x in (query, key, value)]\n",
      "        \n",
      "        # Compute attention scores and apply softmax\n",
      "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(embed_dim // self.num_heads)\n",
      "        attention_weights = F.softmax(scores, dim=-1)\n",
      "        \n",
      "        # Apply the attention weights to the value tensor\n",
      "        context_representation = torch.matmul(attention_weights, value).transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
      "        return context_representation\n",
      "```\n",
      "\n",
      "### 3. Explore Real-World Applications\n",
      "\n",
      "Self-attention has proven useful in various applications such as machine translation, text summarization, and sentiment analysis. Implement these use cases to see the practical benefits of self-attention.\n",
      "\n",
      "### 4. Read Research Papers and Case Studies\n",
      "\n",
      "Stay up-to-date with the latest advancements by reading research papers like \"Attention is All You Need\" (Vaswani et al., 2017) or case studies in top journals such as NeurIPS, ICLR, and EMNLP.\n",
      "\n",
      "### 5. Join Online Communities and Forums\n",
      "\n",
      "Engage with other developers and researchers through forums like Reddit’s r/MachineLearning, Stack Overflow, or GitHub discussions related to NLP projects. This can provide valuable insights and feedback.\n",
      "\n",
      "By following these steps, you will be well-equipped to leverage self-attention in your projects and contribute to the advancement of natural language processing technologies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1dba81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
